{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, Transform, and Load Bank Data\n",
    "The project uses the ETL process to create a list of the top top banks in the world based on market capital. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Required Libraries\n",
    "The libraries used in this project are:\n",
    "- pandas: to use a dataframe for easy transformation\n",
    "- requests: to get the data from the web\n",
    "- bs4: to scrape the required data\n",
    "- sqlite3: to create and query a database\n",
    "- numpy\n",
    "- datetime: for logging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sqlite3\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://web.archive.org/web/20230908091635/https://en.wikipedia.org/wiki/List_of_largest_banks'\n",
    "db_name = 'Banks.db'\n",
    "table_name = 'Largest_banks'\n",
    "output_file = 'largest_banks_data.csv'\n",
    "table_attributes = ['Name', 'MC_USD_Billion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Message\n",
    "Keep track of each operation in the ETL process. The following log function provides a timestamp of each operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file to store the logs\n",
    "log_file = 'code_log.txt'\n",
    "\n",
    "# Log message function\n",
    "def log_progress(message):\n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S'\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Convert time to a string\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "\n",
    "    # Log the message to the file\n",
    "    with open(log_file, 'a') as file_log:\n",
    "        file_log.write(f'{timestamp} : {message}\\n')\n",
    "\n",
    "# Declare the completion of the set up\n",
    "log_progress('Preliminaries complete. Initiating ETL process')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Extraction Stage\n",
    "In this stage the data is extracted from the web and loaded into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Name  MC_USD_Billion\n",
      "0                           JPMorgan Chase          432.92\n",
      "1                          Bank of America          231.52\n",
      "2  Industrial and Commercial Bank of China          194.56\n",
      "3               Agricultural Bank of China          160.68\n",
      "4                                HDFC Bank          157.91\n",
      "5                              Wells Fargo          155.87\n",
      "6                        HSBC Holdings PLC          148.90\n",
      "7                           Morgan Stanley          140.83\n",
      "8                  China Construction Bank          139.82\n",
      "9                            Bank of China          136.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/64/w7ll3frd5ll8jdd45h0041pw0000gn/T/ipykernel_4178/1123685148.py:31: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data_frame = pd.concat([data_frame, temp_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Extraction function\n",
    "def extraction(url, table_attr):\n",
    "    data_frame = pd.DataFrame(columns=table_attr)\n",
    "    \n",
    "    try:\n",
    "        # Load the web page as an HTML document\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML document\n",
    "            html_content = response.text\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            # Locate the specifeid table after the text: By market Capitalization\n",
    "            span = soup.find('span', id='By_market_capitalization')\n",
    "            h2_tag = span.find_parent('h2')\n",
    "            table = h2_tag.find_next('table')\n",
    "\n",
    "            # Find the rows of the table\n",
    "            rows = table.find_all('tr')\n",
    "            \n",
    "            # Iterate the rows to extract the bank name and the associated market capital\n",
    "            for row in rows:\n",
    "                columns = row.find_all('td')\n",
    "                if len(columns) > 0:\n",
    "                    name = columns[1].get_text(strip=True)\n",
    "                    market_capital = columns[2].get_text(strip=True)\n",
    "\n",
    "                    # Place data in a dictionary and append it to the DF\n",
    "                    bank_dict = {table_attr[0]: name, table_attr[1]: float(market_capital)}\n",
    "                    temp_df = pd.DataFrame(bank_dict, index=[0])\n",
    "                    data_frame = pd.concat([data_frame, temp_df], ignore_index=True)\n",
    "            return data_frame     \n",
    "    except Exception as e:\n",
    "        print(f'Error loading the page: {e}')\n",
    "        return None   \n",
    "            \n",
    "\n",
    "bank_data_frame = extraction(url, table_attributes)\n",
    "# Log the extraction phase completion\n",
    "log_progress('Data extraction complete. Initiating Transformation process')\n",
    "print(bank_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transform Stage\n",
    "The data in the dataframe from the previous step is transformed in this stage to meet specific requirements:\n",
    "- Add 3 new columns to sure 3 more currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Name  MC_USD_Billion  MC_GBP_Billion  \\\n",
      "0                           JPMorgan Chase          432.92          346.34   \n",
      "1                          Bank of America          231.52          185.22   \n",
      "2  Industrial and Commercial Bank of China          194.56          155.65   \n",
      "3               Agricultural Bank of China          160.68          128.54   \n",
      "4                                HDFC Bank          157.91          126.33   \n",
      "5                              Wells Fargo          155.87          124.70   \n",
      "6                        HSBC Holdings PLC          148.90          119.12   \n",
      "7                           Morgan Stanley          140.83          112.66   \n",
      "8                  China Construction Bank          139.82          111.86   \n",
      "9                            Bank of China          136.81          109.45   \n",
      "\n",
      "   MC_EUR_Billion  MC_INR_Billion  \n",
      "0          402.62        35910.71  \n",
      "1          215.31        19204.58  \n",
      "2          180.94        16138.75  \n",
      "3          149.43        13328.41  \n",
      "4          146.86        13098.63  \n",
      "5          144.96        12929.42  \n",
      "6          138.48        12351.26  \n",
      "7          130.97        11681.85  \n",
      "8          130.03        11598.07  \n",
      "9          127.23        11348.39  \n",
      "146.86\n"
     ]
    }
   ],
   "source": [
    "# Transformation function\n",
    "def transformation(data, csv_path):\n",
    "    # Convert the contents of the csv file to a dictionary\n",
    "    exchange_rate_df = pd.read_csv(csv_path)\n",
    "    exchange_rate_dict = exchange_rate_df.set_index('Currency').to_dict()['Rate']\n",
    "    \n",
    "    # Add 3 columns to the data frame\n",
    "    data['MC_GBP_Billion'] = [\n",
    "        np.round(value * exchange_rate_dict['GBP'], 2) for value in data['MC_USD_Billion']]\n",
    "    data['MC_EUR_Billion'] = [\n",
    "        np.round(value * exchange_rate_dict['EUR'], 2) for value in data['MC_USD_Billion']]\n",
    "    data['MC_INR_Billion'] = [\n",
    "        np.round(value * exchange_rate_dict['INR'], 2) for value in data['MC_USD_Billion']]\n",
    "    return data\n",
    "\n",
    "cleaned_data = transformation(bank_data_frame, 'exchange_rate.csv')\n",
    "# Log the transformation phase completion\n",
    "log_progress('Data transformation complete. Initiating Loading process')\n",
    "print(cleaned_data)\n",
    "\n",
    "print(cleaned_data['MC_EUR_Billion'][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Load Stage\n",
    "The data will loaded loaded to a csv file as well as a database for storage. This will allows other users to further explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to a csv file\n",
    "def load_data_to_csv(data, output_path):\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "stored_file = load_data_to_csv(cleaned_data, output_file)\n",
    "# Log the completion of the data being loaded to a csv file\n",
    "log_progress('Data saved to CSV file')\n",
    "\n",
    "# Load data to a sqlite database\n",
    "def load_data_to_sqlite(data, sql_con, table_name):\n",
    "    \n",
    "    # Add the table to the database\n",
    "    data.to_sql(table_name, sql_con, if_exists='replace', index=False)\n",
    "    \n",
    "# Log initiated sqlite connection\n",
    "sql_connection = sqlite3.connect(db_name)\n",
    "log_progress('SQL Connection initiated')\n",
    "bank_sql_db = load_data_to_sqlite(cleaned_data, sql_connection, table_name)\n",
    "\n",
    "# Log the completion of the data being loaded to a sqlite database\n",
    "log_progress('Data loaded to Database as a table, Executing queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Database\n",
    "Run queries to ensure that the data was stored properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Content:\n",
      "                                       Name  MC_USD_Billion  MC_GBP_Billion  \\\n",
      "0                           JPMorgan Chase          432.92          346.34   \n",
      "1                          Bank of America          231.52          185.22   \n",
      "2  Industrial and Commercial Bank of China          194.56          155.65   \n",
      "3               Agricultural Bank of China          160.68          128.54   \n",
      "4                                HDFC Bank          157.91          126.33   \n",
      "5                              Wells Fargo          155.87          124.70   \n",
      "6                        HSBC Holdings PLC          148.90          119.12   \n",
      "7                           Morgan Stanley          140.83          112.66   \n",
      "8                  China Construction Bank          139.82          111.86   \n",
      "9                            Bank of China          136.81          109.45   \n",
      "\n",
      "   MC_EUR_Billion  MC_INR_Billion  \n",
      "0          402.62        35910.71  \n",
      "1          215.31        19204.58  \n",
      "2          180.94        16138.75  \n",
      "3          149.43        13328.41  \n",
      "4          146.86        13098.63  \n",
      "5          144.96        12929.42  \n",
      "6          138.48        12351.26  \n",
      "7          130.97        11681.85  \n",
      "8          130.03        11598.07  \n",
      "9          127.23        11348.39  \n",
      "\n",
      "Average Market Capital:    AVG(MC_GBP_Billion)\n",
      "0              151.987\n",
      "\n",
      "Top 5 Banks:                                       Name\n",
      "0                           JPMorgan Chase\n",
      "1                          Bank of America\n",
      "2  Industrial and Commercial Bank of China\n",
      "3               Agricultural Bank of China\n",
      "4                                HDFC Bank\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_query(query, sql_connection):\n",
    "    query_result = pd.read_sql(query, sql_connection)\n",
    "    return query_result\n",
    "\n",
    "\n",
    "# Make 3 query calls\n",
    "query_all_contents = 'SELECT * FROM Largest_banks'\n",
    "result_all_contents = run_query(query_all_contents, sql_connection)\n",
    "print(f'Full Content:\\n {result_all_contents}\\n')\n",
    "\n",
    "# Print average MC_USD_Billion\n",
    "avg_billion = 'SELECT AVG(MC_GBP_Billion) FROM Largest_banks'\n",
    "result_avg_billion = run_query(avg_billion, sql_connection)\n",
    "print(f'Average Market Capital: {result_avg_billion}\\n')\n",
    "\n",
    "# Print the top 5 banks\n",
    "query_top_5 = 'SELECT Name from Largest_banks LIMIT 5'\n",
    "result_top_5 = run_query(query_top_5, sql_connection)\n",
    "print(f'Top 5 Banks: {result_top_5}\\n')\n",
    "\n",
    "# Log the completion of the query execution\n",
    "log_progress('Process Complete')\n",
    "\n",
    "# Log close of the connection\n",
    "sql_connection.close()\n",
    "log_progress('Server Connection closed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
